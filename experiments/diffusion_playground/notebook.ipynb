{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb763ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusion_co_design.pretrain.rware.transform import storage_to_layout\n",
    "from diffusion_co_design.bin.train_rware import TrainingConfig, ScenarioConfig\n",
    "from diffusion_co_design.utils import (\n",
    "    OUTPUT_DIR,\n",
    "    omega_to_pydantic,\n",
    "    get_latest_model,\n",
    "    cuda,\n",
    ")\n",
    "from diffusion_co_design.pretrain.rware.graph import WarehouseGNNBase, E3GNNLayer\n",
    "from guided_diffusion.script_util import create_classifier, classifier_defaults\n",
    "from diffusion_co_design.pretrain.rware.generator import (\n",
    "    Generator,\n",
    "    OptimizerDetails,\n",
    ")\n",
    "from rware.warehouse import Warehouse\n",
    "\n",
    "from dataset import (\n",
    "    load_dataset,\n",
    "    CollateFn,\n",
    "    ImageCollateFn,\n",
    "    working_dir,\n",
    ")\n",
    "\n",
    "device = cuda\n",
    "training_dir = \"/home/markhaoxiang/.diffusion_co_design/training/2025-04-05/04-00-12\"  # Four corners\n",
    "\n",
    "# Load latest model and config\n",
    "checkpoint_dir = os.path.join(training_dir, \"checkpoints\")\n",
    "latest_policy = get_latest_model(checkpoint_dir, \"policy_\")\n",
    "# Get config\n",
    "hydra_dir = os.path.join(training_dir, \".hydra\")\n",
    "cfg = omega_to_pydantic(\n",
    "    OmegaConf.load(os.path.join(hydra_dir, \"config.yaml\")), TrainingConfig\n",
    ")\n",
    "\n",
    "diffusion_dir = pretrain_dir = os.path.join(\n",
    "    OUTPUT_DIR, \"diffusion_pretrain\", \"graph\", cfg.scenario.name\n",
    ")\n",
    "latest_checkpoint = get_latest_model(diffusion_dir, \"model\")\n",
    "\n",
    "cfg.scenario.representation = \"image\"\n",
    "# Make Dataset\n",
    "train_dataset, eval_dataset = load_dataset(\n",
    "    scenario=cfg.scenario,\n",
    "    training_dir=training_dir,\n",
    "    dataset_size=10_000,\n",
    "    num_workers=25,\n",
    "    test_proportion=0.2,\n",
    "    recompute=False,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a089b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "collate_fn = CollateFn(cfg.scenario, device)\n",
    "collate_fn_image = ImageCollateFn(cfg.scenario, device)\n",
    "\n",
    "\n",
    "def make_dataloader(dataset, batch_size=128, is_image: bool = False):\n",
    "    cf = collate_fn if not is_image else collate_fn_image\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        collate_fn=cf,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataloader = make_dataloader(train_dataset, batch_size=128)\n",
    "eval_dataloader = make_dataloader(eval_dataset, batch_size=128)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4237cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\n",
    "#     \"/home/markhaoxiang/.diffusion_co_design/experiments/diffusion_playground/gnn-cnn_graph/2025-04-06 17-40-02/checkpoints/classifier.pt\",\n",
    "#     weights_only=False,\n",
    "# )\n",
    "\n",
    "model = torch.load(\n",
    "    \"/home/markhaoxiang/.diffusion_co_design/experiments/diffusion_playground/gnn-cnn_graph/2025-04-06 20-41-44/checkpoints/classifier.pt\",\n",
    "    weights_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c677a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE_SIZE_CNST = 2.5\n",
    "\n",
    "# test_layout = [next(iter(train_dataset))]\n",
    "# test_layout, _ = collate_fn(test_layout)\n",
    "# pos, color = test_layout\n",
    "\n",
    "# pos.requires_grad = True\n",
    "# pos_optim = torch.optim.Adam([pos], lr=0.01)\n",
    "\n",
    "\n",
    "# n_iterations = 500\n",
    "# for iteration in range(n_iterations):\n",
    "#     pos_optim.zero_grad()\n",
    "#     y_pred = model.predict((pos, color))\n",
    "#     loss = -y_pred.mean()\n",
    "#     loss.backward()\n",
    "#     pos_optim.step()\n",
    "\n",
    "#     if iteration % (n_iterations // 10) == 0:\n",
    "#         print(f\"Iteration {iteration} Loss: {loss.item()}\")\n",
    "\n",
    "#         fig, ax = plt.subplots(figsize=(FIGURE_SIZE_CNST, FIGURE_SIZE_CNST))\n",
    "\n",
    "#         show_pos = (pos.squeeze() + 1) / 2\n",
    "#         show_pos = show_pos * cfg.scenario.size\n",
    "#         layout = storage_to_layout(\n",
    "#             features=show_pos.numpy(force=True),\n",
    "#             config=cfg.scenario,\n",
    "#             representation_override=\"graph\",\n",
    "#         )\n",
    "#         warehouse = Warehouse(layout=layout, render_mode=\"rgb_array\")\n",
    "#         print(len(warehouse.shelves))\n",
    "#         im = warehouse.render()\n",
    "#         ax.imshow(im)\n",
    "#         ax.axis(\"off\")\n",
    "#         plt.show()\n",
    "#         warehouse.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cba9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_constraint(x):\n",
    "    B, N, _ = x.shape\n",
    "    for _ in range(16):\n",
    "        x = x.clamp(min=-1, max=1)\n",
    "        diff = x[:, :, None, :] - x[:, None, :, :]  # [B, N, N, 2]\n",
    "        dist = torch.norm(diff, dim=-1) + 1e-6  # [B, N, N]\n",
    "\n",
    "        mask = torch.eye(N, device=x.device).bool()\n",
    "        dist.masked_fill_(mask[None], float(\"inf\"))\n",
    "\n",
    "        min_dist = 2 / cfg.scenario.size\n",
    "        violation = dist < min_dist\n",
    "        force = (min_dist - dist).clamp(min=0.0)\n",
    "        direction = diff / dist.unsqueeze(-1)\n",
    "\n",
    "        disp = (force.unsqueeze(-1) * direction) * violation.unsqueeze(-1)\n",
    "\n",
    "        total_disp = disp.sum(dim=2)  # [B, N, 2]\n",
    "        x = x + total_disp\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def projection_constraint_2(x):\n",
    "    B, N, _ = x.shape\n",
    "    G = cfg.scenario.size\n",
    "    device = x.device\n",
    "\n",
    "    lin = torch.linspace(-1, 1, steps=G, device=device)\n",
    "    yy, xx = torch.meshgrid(lin, lin, indexing=\"ij\")\n",
    "\n",
    "    grid = torch.stack([xx, yy], dim=-1).reshape(-1, 2)  # [GÂ², 2]\n",
    "\n",
    "    assert N <= grid.shape[0], \"More points than available grid cells\"\n",
    "\n",
    "    distance_limit = 1 / (G - 1) - 1e-5\n",
    "\n",
    "    target = []\n",
    "    for b in range(B):\n",
    "        x_b = x[b]\n",
    "\n",
    "        cost = (\n",
    "            torch.cdist(x_b.unsqueeze(0), grid.unsqueeze(0), p=1)\n",
    "            .squeeze(0)\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        _, col_ind = linear_sum_assignment(cost[:, :])\n",
    "\n",
    "        matched = grid[col_ind]\n",
    "        target.append(matched)\n",
    "\n",
    "    target = torch.stack(target, dim=0).to(device)\n",
    "    delta = target - x\n",
    "    original_sign = torch.sign(delta)\n",
    "    delta = delta - distance_limit * original_sign\n",
    "    new_sign = torch.sign(delta)\n",
    "    delta = delta * (original_sign == new_sign).float()\n",
    "    target = x + delta\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "test = torch.rand(8, 50, 2)\n",
    "x = projection_constraint_2(test) * cfg.scenario.size\n",
    "diff = x[:, :, None, :] - x[:, None, :, :]\n",
    "dist = torch.norm(diff, dim=-1) + 1e-6\n",
    "mask = torch.eye(50, device=x.device).bool()\n",
    "dist.masked_fill_(mask[None], float(\"inf\"))\n",
    "dist.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(\n",
    "    batch_size=10,\n",
    "    generator_model_path=latest_checkpoint,\n",
    "    scenario=cfg.scenario,\n",
    "    guidance_wt=3000,\n",
    "    representation=\"graph\",\n",
    ")\n",
    "guidance_model = model\n",
    "guidance_model.eval()\n",
    "operation = OptimizerDetails()\n",
    "operation.num_recurrences = 16\n",
    "operation.backward_steps = 0\n",
    "operation.projection_constraint = projection_constraint_2\n",
    "\n",
    "\n",
    "def show_batch(environment_batch, n: int = 8):\n",
    "    layouts = []\n",
    "    for image in environment_batch:\n",
    "        layout = storage_to_layout(image, cfg.scenario)\n",
    "        warehouse = Warehouse(layout=layout, render_mode=\"rgb_array\")\n",
    "        layouts.append(warehouse.render())\n",
    "        warehouse.close()\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    axs = axs.ravel()\n",
    "    for ax in axs:\n",
    "        ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        axs[i].imshow(layouts[i])\n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "environment_batch = generator.generate_batch(\n",
    "    value=guidance_model,\n",
    "    use_operation=True,\n",
    "    operation_override=operation,\n",
    ")\n",
    "\n",
    "\n",
    "for env in environment_batch:\n",
    "    cfg.scenario.representation = \"graph\"\n",
    "    layout = storage_to_layout(env, cfg.scenario)\n",
    "    print(len(layout.reset_shelves()))\n",
    "fig, axs = show_batch(environment_batch)\n",
    "fig.suptitle(\"Guided Generation\")\n",
    "fig.tight_layout()\n",
    "\n",
    "X_batch = (\n",
    "    torch.from_numpy(environment_batch).to(device=device, dtype=torch.float32)\n",
    "    # .moveaxis((0, 1, 2, 3), (0, 2, 3, 1))\n",
    ")\n",
    "# X_batch = torch.cat([X_batch, goal_map.unsqueeze(0).expand(8, -1, -1, -1)], dim=1)\n",
    "X_batch = (X_batch / (cfg.scenario.size - 1)) * 2 - 1\n",
    "print(X_batch.shape)\n",
    "print(guidance_model(X_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676036f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(environment_batch)\n",
    "diff = x[:, :, None, :] - x[:, None, :, :]\n",
    "dist = torch.norm(diff, dim=-1) + 1e-6\n",
    "mask = torch.eye(50, device=x.device).bool()\n",
    "dist.masked_fill_(mask[None], float(\"inf\"))\n",
    "dist.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a388193",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ad4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphClassifier(WarehouseGNNBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scenario: ScenarioConfig,\n",
    "        node_embedding_dim: int = 128,\n",
    "        edge_embedding_dim: int = 32,\n",
    "        num_layers: int = 5,\n",
    "        use_radius_graph: bool = True,\n",
    "        radius: float = 0.5,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            scenario=scenario,\n",
    "            use_radius_graph=use_radius_graph,\n",
    "            radius=radius,\n",
    "            include_color_features=True,\n",
    "        )\n",
    "\n",
    "        self.embedding_dim = node_embedding_dim\n",
    "        self.num_nodes = scenario.n_goals + scenario.n_shelves\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.h_in = nn.Linear(self.feature_dim, node_embedding_dim)\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(\n",
    "                E3GNNLayer(\n",
    "                    node_embedding_dim=node_embedding_dim,\n",
    "                    edge_embedding_dim=edge_embedding_dim,\n",
    "                    graph_embedding_dim=0,  # no timestep embeddings\n",
    "                    update_node_features=i < num_layers - 1,\n",
    "                    use_attention=True,\n",
    "                    normalise_pos=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.readout = global_add_pool\n",
    "        self.out_mlp = nn.Sequential(\n",
    "            nn.Linear(node_embedding_dim, node_embedding_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(node_embedding_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, pos: torch.Tensor, color: torch.Tensor) -> torch.Tensor:\n",
    "        graph, _ = self.make_graph_from_data(pos, color=color)\n",
    "        h = self.h_in(graph.h)  # [N, d]\n",
    "        pos = graph.pos  # [N, 2]\n",
    "        batch = graph.batch  # [N]\n",
    "\n",
    "        for i, gnn in enumerate(self.convs):\n",
    "            h, pos = gnn(h, graph.edge_index, pos, None, batch)\n",
    "\n",
    "        # Readout across entire graph (goals + shelves)\n",
    "        graph_repr = self.readout(h, batch)\n",
    "        return self.out_mlp(graph_repr).squeeze(-1)\n",
    "\n",
    "\n",
    "model = GraphClassifier(cfg.scenario).to(device)\n",
    "\n",
    "# Test\n",
    "(pos, color), y = next(iter(train_dataloader))\n",
    "\n",
    "number_parameters = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Number of parameters: {number_parameters}\")\n",
    "assert model(pos, color).shape == y.shape\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c690a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NUM_EPOCHS = 50\n",
    "RECOMPUTE = True\n",
    "\n",
    "model = GraphClassifier(cfg.scenario).to(device)\n",
    "model_dir = os.path.join(working_dir, \"classifier_gnn.pt\")\n",
    "\n",
    "\n",
    "if RECOMPUTE or not os.path.exists(model_dir):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=3e-5, weight_decay=0.05)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    with tqdm(range(TRAIN_NUM_EPOCHS)) as pbar:\n",
    "        for epoch in range(TRAIN_NUM_EPOCHS):\n",
    "            running_train_loss = 0\n",
    "            model.train()\n",
    "            for (pos, colors), y in train_dataloader:\n",
    "                optim.zero_grad()\n",
    "\n",
    "                batch_size = pos.shape[0]\n",
    "                y_pred = model(pos, colors)\n",
    "                loss = criterion(y_pred.view(batch_size, -1), y.view(batch_size, -1))\n",
    "\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                running_train_loss += loss.item()\n",
    "            running_train_loss = running_train_loss / len(train_dataloader)\n",
    "\n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            running_eval_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for (pos, colors), y in eval_dataloader:\n",
    "                    y_pred = model(pos, colors).squeeze()\n",
    "                    batch_size = pos.shape[0]\n",
    "                    loss = criterion(\n",
    "                        y_pred.view(batch_size, -1), y.view(batch_size, -1)\n",
    "                    )\n",
    "\n",
    "                    running_eval_loss += loss.item()\n",
    "            running_eval_loss = running_eval_loss / len(eval_dataloader)\n",
    "\n",
    "            train_losses.append(running_train_loss)\n",
    "            eval_losses.append(running_eval_loss)\n",
    "            pbar.set_description(\n",
    "                f\" Train Loss {running_train_loss} Eval Loss {running_eval_loss}\"\n",
    "            )\n",
    "            pbar.update()\n",
    "\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = classifier_defaults()\n",
    "model_dict[\"image_size\"] = cfg.scenario.size\n",
    "model_dict[\"image_channels\"] = cfg.scenario.n_colors\n",
    "\n",
    "model_dict[\"classifier_width\"] = 128\n",
    "model_dict[\"classifier_depth\"] = 2\n",
    "model_dict[\"classifier_attention_resolutions\"] = \"16, 8, 4\"\n",
    "model_dict[\"output_dim\"] = 1\n",
    "\n",
    "model = create_classifier(**model_dict).to(device)\n",
    "number_parameters = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Number of parameters: {number_parameters}\")\n",
    "RECOMPUTE = True\n",
    "TRAIN_NUM_EPOCHS = 50\n",
    "\n",
    "# Train\n",
    "optim = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_image_dataloader = make_dataloader(train_dataset, batch_size=128, is_image=True)\n",
    "eval_image_dataloader = make_dataloader(eval_dataset, batch_size=128, is_image=True)\n",
    "\n",
    "model_dir = os.path.join(working_dir, \"classifier_cnn.pt\")\n",
    "\n",
    "if RECOMPUTE or not os.path.exists(model_dir):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=3e-5, weight_decay=0.05)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    with tqdm(range(TRAIN_NUM_EPOCHS)) as pbar:\n",
    "        for epoch in range(TRAIN_NUM_EPOCHS):\n",
    "            running_train_loss = 0\n",
    "            model.train()\n",
    "            for X, y in train_image_dataloader:\n",
    "                optim.zero_grad()\n",
    "\n",
    "                y_pred = model(X).squeeze()\n",
    "                loss = criterion(y_pred, y)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                running_train_loss += loss.item()\n",
    "            running_train_loss = running_train_loss / len(train_image_dataloader)\n",
    "\n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            running_eval_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for X, y in eval_image_dataloader:\n",
    "                    y_pred = model(X).squeeze()\n",
    "                    loss = criterion(y_pred, y)\n",
    "\n",
    "                    running_eval_loss += loss.item()\n",
    "            running_eval_loss = running_eval_loss / len(eval_image_dataloader)\n",
    "\n",
    "            train_losses.append(running_train_loss)\n",
    "            eval_losses.append(running_eval_loss)\n",
    "            pbar.set_description(\n",
    "                f\" Train Loss {running_train_loss} Eval Loss {running_eval_loss}\"\n",
    "            )\n",
    "            pbar.update()\n",
    "\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
