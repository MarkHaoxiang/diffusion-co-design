{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb763ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusion_co_design.pretrain.rware.transform import (\n",
    "    graph_projection_constraint,\n",
    "    storage_to_layout,\n",
    ")\n",
    "from diffusion_co_design.bin.train_rware import TrainingConfig, ScenarioConfig\n",
    "from diffusion_co_design.utils import (\n",
    "    OUTPUT_DIR,\n",
    "    omega_to_pydantic,\n",
    "    get_latest_model,\n",
    "    cuda,\n",
    ")\n",
    "from diffusion_co_design.pretrain.rware.graph import WarehouseGNNBase, E3GNNLayer\n",
    "from guided_diffusion.script_util import create_classifier, classifier_defaults\n",
    "from diffusion_co_design.pretrain.rware.generator import (\n",
    "    Generator,\n",
    "    OptimizerDetails,\n",
    ")\n",
    "from rware.warehouse import Warehouse\n",
    "\n",
    "from dataset import (\n",
    "    load_dataset,\n",
    "    CollateFn,\n",
    "    ImageCollateFn,\n",
    "    working_dir,\n",
    ")\n",
    "from diffusion_co_design.rware.classifier import GNNCNN\n",
    "\n",
    "device = cuda\n",
    "training_dir = \"/home/markhaoxiang/.diffusion_co_design/training/2025-04-05/04-00-12\"  # Four corners\n",
    "\n",
    "# Load latest model and config\n",
    "checkpoint_dir = os.path.join(training_dir, \"checkpoints\")\n",
    "latest_policy = get_latest_model(checkpoint_dir, \"policy_\")\n",
    "# Get config\n",
    "hydra_dir = os.path.join(training_dir, \".hydra\")\n",
    "cfg = omega_to_pydantic(\n",
    "    OmegaConf.load(os.path.join(hydra_dir, \"config.yaml\")), TrainingConfig\n",
    ")\n",
    "\n",
    "diffusion_dir = pretrain_dir = os.path.join(\n",
    "    OUTPUT_DIR, \"diffusion_pretrain\", \"graph\", cfg.scenario.name\n",
    ")\n",
    "latest_checkpoint = get_latest_model(diffusion_dir, \"model\")\n",
    "\n",
    "cfg.scenario.representation = \"image\"\n",
    "# Make Dataset\n",
    "train_dataset, eval_dataset = load_dataset(\n",
    "    scenario=cfg.scenario,\n",
    "    training_dir=training_dir,\n",
    "    dataset_size=10_000,\n",
    "    num_workers=25,\n",
    "    test_proportion=0.2,\n",
    "    recompute=False,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a089b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "collate_fn = CollateFn(cfg.scenario, device)\n",
    "collate_fn_image = ImageCollateFn(cfg.scenario, device)\n",
    "\n",
    "\n",
    "def make_dataloader(dataset, batch_size=128, is_image: bool = False):\n",
    "    cf = collate_fn if not is_image else collate_fn_image\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        collate_fn=cf,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataloader = make_dataloader(train_dataset, batch_size=128)\n",
    "eval_dataloader = make_dataloader(eval_dataset, batch_size=128)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4237cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_co_design.rware.classifier import make_model\n",
    "\n",
    "cfg.scenario.representation = \"graph\"\n",
    "model = make_model(\"gnn-cnn\", cfg.scenario, model_kwargs={}, device=device)\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"/home/markhaoxiang/.diffusion_co_design/experiments/diffusion_playground/gnn-cnn_graph/2025-04-09 17-24-44/checkpoints/classifier.pt\"\n",
    "    )\n",
    ")\n",
    "assert False\n",
    "\n",
    "# model = torch.load(\n",
    "#     \"/home/markhaoxiang/.diffusion_co_design/experiments/diffusion_playground/gnn-cnn_graph/2025-04-06 20-41-44/checkpoints/classifier.pt\",\n",
    "#     weights_only=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c677a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE_SIZE_CNST = 2.5\n",
    "\n",
    "# test_layout = [next(iter(train_dataset))]\n",
    "# test_layout, _ = collate_fn(test_layout)\n",
    "# pos, color = test_layout\n",
    "\n",
    "# pos.requires_grad = True\n",
    "# pos_optim = torch.optim.Adam([pos], lr=0.01)\n",
    "\n",
    "\n",
    "# n_iterations = 500\n",
    "# for iteration in range(n_iterations):\n",
    "#     pos_optim.zero_grad()\n",
    "#     y_pred = model.predict((pos, color))\n",
    "#     loss = -y_pred.mean()\n",
    "#     loss.backward()\n",
    "#     pos_optim.step()\n",
    "\n",
    "#     if iteration % (n_iterations // 10) == 0:\n",
    "#         print(f\"Iteration {iteration} Loss: {loss.item()}\")\n",
    "\n",
    "#         fig, ax = plt.subplots(figsize=(FIGURE_SIZE_CNST, FIGURE_SIZE_CNST))\n",
    "\n",
    "#         show_pos = (pos.squeeze() + 1) / 2\n",
    "#         show_pos = show_pos * cfg.scenario.size\n",
    "#         layout = storage_to_layout(\n",
    "#             features=show_pos.numpy(force=True),\n",
    "#             config=cfg.scenario,\n",
    "#             representation_override=\"graph\",\n",
    "#         )\n",
    "#         warehouse = Warehouse(layout=layout, render_mode=\"rgb_array\")\n",
    "#         print(len(warehouse.shelves))\n",
    "#         im = warehouse.render()\n",
    "#         ax.imshow(im)\n",
    "#         ax.axis(\"off\")\n",
    "#         plt.show()\n",
    "#         warehouse.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchview import draw_graph\n",
    "\n",
    "# view_model = GNNCNN(cfg.scenario, width=64, depth=2, top_k=5)\n",
    "# model_graph = draw_graph(view_model, input_data=torch.rand(8, 50, 2))\n",
    "# print(sum([p.numel() for p in view_model.parameters()]))\n",
    "# model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model = torch.compile(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(\n",
    "    batch_size=10,\n",
    "    generator_model_path=latest_checkpoint,\n",
    "    scenario=cfg.scenario,\n",
    "    guidance_wt=4,\n",
    "    representation=\"graph\",\n",
    ")\n",
    "# guidance_model = model\n",
    "guidance_model = model\n",
    "guidance_model.eval()\n",
    "operation = OptimizerDetails()\n",
    "operation.lr = 0.01\n",
    "operation.num_recurrences = 8\n",
    "operation.backward_steps = 20\n",
    "operation.projection_constraint = graph_projection_constraint(cfg.scenario)\n",
    "# operation.print = True\n",
    "# operation.print_every = 5\n",
    "# operation.folder = \"test_diffusion\"\n",
    "\n",
    "\n",
    "def show_batch(environment_batch, n: int = 8):\n",
    "    layouts = []\n",
    "    for image in environment_batch:\n",
    "        layout = storage_to_layout(image, cfg.scenario)\n",
    "        warehouse = Warehouse(layout=layout, render_mode=\"rgb_array\")\n",
    "        layouts.append(warehouse.render())\n",
    "        warehouse.close()\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    axs = axs.ravel()\n",
    "    for ax in axs:\n",
    "        ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        axs[i].imshow(layouts[i])\n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "environment_batch = generator.generate_batch(\n",
    "    value=guidance_model,\n",
    "    use_operation=True,\n",
    "    operation_override=operation,\n",
    ")\n",
    "\n",
    "\n",
    "cfg.scenario.representation = \"graph\"\n",
    "for env in environment_batch:\n",
    "    layout = storage_to_layout(env, cfg.scenario)\n",
    "    print(len(layout.reset_shelves()))\n",
    "fig, axs = show_batch(environment_batch)\n",
    "fig.suptitle(\"Guided Generation\")\n",
    "fig.tight_layout()\n",
    "\n",
    "X_batch = (\n",
    "    torch.from_numpy(environment_batch).to(device=device, dtype=torch.float32)\n",
    "    # .moveaxis((0, 1, 2, 3), (0, 2, 3, 1))\n",
    ")\n",
    "# X_batch = torch.cat([X_batch, goal_map.unsqueeze(0).expand(8, -1, -1, -1)], dim=1)\n",
    "X_batch = (X_batch / (cfg.scenario.size - 1)) * 2 - 1\n",
    "print(X_batch.shape)\n",
    "print(guidance_model(X_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b21ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2736cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\n",
    "    \"/home/markhaoxiang/.diffusion_co_design/experiments/diffusion_playground/unet-cnn_image/2025-04-06 14-39-45/checkpoints/classifier.pt\",\n",
    "    weights_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eb9b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_diffusion_dir = os.path.join(\n",
    "    OUTPUT_DIR, \"diffusion_pretrain\", \"image\", cfg.scenario.name\n",
    ")\n",
    "latest_image_checkpoint = get_latest_model(image_diffusion_dir, \"model\")\n",
    "\n",
    "generator = Generator(\n",
    "    batch_size=10,\n",
    "    generator_model_path=latest_image_checkpoint,\n",
    "    scenario=cfg.scenario,\n",
    "    guidance_wt=200,\n",
    "    representation=\"image\",\n",
    ")\n",
    "guidance_model = model\n",
    "guidance_model.eval()\n",
    "operation = OptimizerDetails()\n",
    "operation.num_recurrences = 8\n",
    "operation.backward_steps = 0\n",
    "\n",
    "environment_batch = generator.generate_batch(\n",
    "    value=guidance_model,\n",
    "    use_operation=True,\n",
    "    operation_override=operation,\n",
    ")\n",
    "\n",
    "\n",
    "cfg.scenario.representation = \"image\"\n",
    "for env in environment_batch:\n",
    "    layout = storage_to_layout(env, cfg.scenario)\n",
    "    print(len(layout.reset_shelves()))\n",
    "fig, axs = show_batch(environment_batch)\n",
    "fig.suptitle(\"Guided Generation\")\n",
    "fig.tight_layout()\n",
    "\n",
    "X_batch = (\n",
    "    torch.from_numpy(environment_batch).to(device=device, dtype=torch.float32)\n",
    "    # .moveaxis((0, 1, 2, 3), (0, 2, 3, 1))\n",
    ")\n",
    "# X_batch = torch.cat([X_batch, goal_map.unsqueeze(0).expand(8, -1, -1, -1)], dim=1)\n",
    "X_batch = X_batch * 2 - 1\n",
    "print(X_batch.shape)\n",
    "print(guidance_model(X_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ad4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphClassifier(WarehouseGNNBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scenario: ScenarioConfig,\n",
    "        node_embedding_dim: int = 128,\n",
    "        edge_embedding_dim: int = 32,\n",
    "        num_layers: int = 5,\n",
    "        use_radius_graph: bool = True,\n",
    "        radius: float = 0.5,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            scenario=scenario,\n",
    "            use_radius_graph=use_radius_graph,\n",
    "            radius=radius,\n",
    "            include_color_features=True,\n",
    "        )\n",
    "\n",
    "        self.embedding_dim = node_embedding_dim\n",
    "        self.num_nodes = scenario.n_goals + scenario.n_shelves\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.h_in = nn.Linear(self.feature_dim, node_embedding_dim)\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(\n",
    "                E3GNNLayer(\n",
    "                    node_embedding_dim=node_embedding_dim,\n",
    "                    edge_embedding_dim=edge_embedding_dim,\n",
    "                    graph_embedding_dim=0,  # no timestep embeddings\n",
    "                    update_node_features=i < num_layers - 1,\n",
    "                    use_attention=True,\n",
    "                    normalise_pos=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.readout = global_add_pool\n",
    "        self.out_mlp = nn.Sequential(\n",
    "            nn.Linear(node_embedding_dim, node_embedding_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(node_embedding_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, pos: torch.Tensor, color: torch.Tensor) -> torch.Tensor:\n",
    "        graph, _ = self.make_graph_from_data(pos, color=color)\n",
    "        h = self.h_in(graph.h)  # [N, d]\n",
    "        pos = graph.pos  # [N, 2]\n",
    "        batch = graph.batch  # [N]\n",
    "\n",
    "        for i, gnn in enumerate(self.convs):\n",
    "            h, pos = gnn(h, graph.edge_index, pos, None, batch)\n",
    "\n",
    "        # Readout across entire graph (goals + shelves)\n",
    "        graph_repr = self.readout(h, batch)\n",
    "        return self.out_mlp(graph_repr).squeeze(-1)\n",
    "\n",
    "\n",
    "model = GraphClassifier(cfg.scenario).to(device)\n",
    "\n",
    "# Test\n",
    "(pos, color), y = next(iter(train_dataloader))\n",
    "\n",
    "number_parameters = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Number of parameters: {number_parameters}\")\n",
    "assert model(pos, color).shape == y.shape\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c690a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NUM_EPOCHS = 50\n",
    "RECOMPUTE = True\n",
    "\n",
    "model = GraphClassifier(cfg.scenario).to(device)\n",
    "model_dir = os.path.join(working_dir, \"classifier_gnn.pt\")\n",
    "\n",
    "\n",
    "if RECOMPUTE or not os.path.exists(model_dir):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=3e-5, weight_decay=0.05)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    with tqdm(range(TRAIN_NUM_EPOCHS)) as pbar:\n",
    "        for epoch in range(TRAIN_NUM_EPOCHS):\n",
    "            running_train_loss = 0\n",
    "            model.train()\n",
    "            for (pos, colors), y in train_dataloader:\n",
    "                optim.zero_grad()\n",
    "\n",
    "                batch_size = pos.shape[0]\n",
    "                y_pred = model(pos, colors)\n",
    "                loss = criterion(y_pred.view(batch_size, -1), y.view(batch_size, -1))\n",
    "\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                running_train_loss += loss.item()\n",
    "            running_train_loss = running_train_loss / len(train_dataloader)\n",
    "\n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            running_eval_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for (pos, colors), y in eval_dataloader:\n",
    "                    y_pred = model(pos, colors).squeeze()\n",
    "                    batch_size = pos.shape[0]\n",
    "                    loss = criterion(\n",
    "                        y_pred.view(batch_size, -1), y.view(batch_size, -1)\n",
    "                    )\n",
    "\n",
    "                    running_eval_loss += loss.item()\n",
    "            running_eval_loss = running_eval_loss / len(eval_dataloader)\n",
    "\n",
    "            train_losses.append(running_train_loss)\n",
    "            eval_losses.append(running_eval_loss)\n",
    "            pbar.set_description(\n",
    "                f\" Train Loss {running_train_loss} Eval Loss {running_eval_loss}\"\n",
    "            )\n",
    "            pbar.update()\n",
    "\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = classifier_defaults()\n",
    "model_dict[\"image_size\"] = cfg.scenario.size\n",
    "model_dict[\"image_channels\"] = cfg.scenario.n_colors\n",
    "\n",
    "model_dict[\"classifier_width\"] = 128\n",
    "model_dict[\"classifier_depth\"] = 2\n",
    "model_dict[\"classifier_attention_resolutions\"] = \"16, 8, 4\"\n",
    "model_dict[\"output_dim\"] = 1\n",
    "\n",
    "model = create_classifier(**model_dict).to(device)\n",
    "number_parameters = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Number of parameters: {number_parameters}\")\n",
    "RECOMPUTE = True\n",
    "TRAIN_NUM_EPOCHS = 50\n",
    "\n",
    "# Train\n",
    "optim = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_image_dataloader = make_dataloader(train_dataset, batch_size=128, is_image=True)\n",
    "eval_image_dataloader = make_dataloader(eval_dataset, batch_size=128, is_image=True)\n",
    "\n",
    "model_dir = os.path.join(working_dir, \"classifier_cnn.pt\")\n",
    "\n",
    "if RECOMPUTE or not os.path.exists(model_dir):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=3e-5, weight_decay=0.05)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    with tqdm(range(TRAIN_NUM_EPOCHS)) as pbar:\n",
    "        for epoch in range(TRAIN_NUM_EPOCHS):\n",
    "            running_train_loss = 0\n",
    "            model.train()\n",
    "            for X, y in train_image_dataloader:\n",
    "                optim.zero_grad()\n",
    "\n",
    "                y_pred = model(X).squeeze()\n",
    "                loss = criterion(y_pred, y)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                running_train_loss += loss.item()\n",
    "            running_train_loss = running_train_loss / len(train_image_dataloader)\n",
    "\n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            running_eval_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for X, y in eval_image_dataloader:\n",
    "                    y_pred = model(X).squeeze()\n",
    "                    loss = criterion(y_pred, y)\n",
    "\n",
    "                    running_eval_loss += loss.item()\n",
    "            running_eval_loss = running_eval_loss / len(eval_image_dataloader)\n",
    "\n",
    "            train_losses.append(running_train_loss)\n",
    "            eval_losses.append(running_eval_loss)\n",
    "            pbar.set_description(\n",
    "                f\" Train Loss {running_train_loss} Eval Loss {running_eval_loss}\"\n",
    "            )\n",
    "            pbar.update()\n",
    "\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
