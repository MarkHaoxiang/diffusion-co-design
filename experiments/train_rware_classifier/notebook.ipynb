{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb763ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusion_co_design.pretrain.rware.transform import (\n",
    "    graph_projection_constraint,\n",
    "    storage_to_layout,\n",
    ")\n",
    "from experiments.train_rware.main import TrainingConfig, ScenarioConfig\n",
    "from diffusion_co_design.common import (\n",
    "    OUTPUT_DIR,\n",
    "    omega_to_pydantic,\n",
    "    get_latest_model,\n",
    "    cuda,\n",
    ")\n",
    "from diffusion_co_design.pretrain.rware.graph import WarehouseGNNBase, E3GNNLayer\n",
    "from guided_diffusion.script_util import create_classifier, classifier_defaults\n",
    "from diffusion_co_design.pretrain.rware.generator import (\n",
    "    Generator,\n",
    "    OptimizerDetails,\n",
    ")\n",
    "from rware.warehouse import Warehouse\n",
    "\n",
    "from dataset import (\n",
    "    load_dataset,\n",
    "    make_dataloader,\n",
    "    CollateFn,\n",
    "    ImageCollateFn,\n",
    "    working_dir,\n",
    ")\n",
    "from diffusion_co_design.rware.model.classifier import GNNCNN\n",
    "\n",
    "device = cuda\n",
    "training_dir = \"/home/markhaoxiang/.diffusion_co_design/training/2025-04-05/04-00-12\"  # Four corners\n",
    "\n",
    "# Load latest model and config\n",
    "checkpoint_dir = os.path.join(training_dir, \"checkpoints\")\n",
    "latest_policy = get_latest_model(checkpoint_dir, \"policy_\")\n",
    "# Get config\n",
    "hydra_dir = os.path.join(training_dir, \".hydra\")\n",
    "cfg = omega_to_pydantic(\n",
    "    OmegaConf.load(os.path.join(hydra_dir, \"config.yaml\")), TrainingConfig\n",
    ")\n",
    "\n",
    "diffusion_dir = pretrain_dir = os.path.join(\n",
    "    OUTPUT_DIR, \"diffusion_pretrain\", \"graph\", cfg.scenario.name\n",
    ")\n",
    "latest_checkpoint = get_latest_model(diffusion_dir, \"model\")\n",
    "\n",
    "# Make Dataset\n",
    "train_dataset, eval_dataset = load_dataset(\n",
    "    scenario=cfg.scenario,\n",
    "    training_dir=training_dir,\n",
    "    dataset_size=10_000,\n",
    "    num_workers=25,\n",
    "    test_proportion=0.2,\n",
    "    recompute=False,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a089b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataloader = make_dataloader(\n",
    "    train_dataset,\n",
    "    scenario=cfg.scenario,\n",
    "    batch_size=128,\n",
    "    representation=\"graph\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "eval_dataloader = make_dataloader(\n",
    "    eval_dataset,\n",
    "    scenario=cfg.scenario,\n",
    "    batch_size=128,\n",
    "    representation=\"graph\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4237cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_co_design.rware.model.classifier import make_model\n",
    "\n",
    "cfg.scenario.representation = \"graph\"\n",
    "model = make_model(\"gnn-cnn\", cfg.scenario, model_kwargs={}, device=device)\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"/home/markhaoxiang/.diffusion_co_design/training/2025-04-09/18-44-16/checkpoints/designer_1900.pt\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# model = torch.load(\n",
    "#     \"/home/markhaoxiang/.diffusion_co_design/experiments/diffusion_playground/gnn-cnn_graph/2025-04-06 20-41-44/checkpoints/classifier.pt\",\n",
    "#     weights_only=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c677a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE_SIZE_CNST = 2.5\n",
    "\n",
    "# test_layout = [next(iter(train_dataset))]\n",
    "# test_layout, _ = collate_fn(test_layout)\n",
    "# pos, color = test_layout\n",
    "\n",
    "# pos.requires_grad = True\n",
    "# pos_optim = torch.optim.Adam([pos], lr=0.01)\n",
    "\n",
    "# constraint = graph_projection_constraint(cfg.scenario)\n",
    "\n",
    "# n_iterations = 1000\n",
    "# for iteration in range(n_iterations):\n",
    "#     pos.requires_grad = True\n",
    "#     pos_optim.zero_grad()\n",
    "#     y_pred = model.predict((pos, color))\n",
    "#     loss = -y_pred.mean()\n",
    "#     loss.backward()\n",
    "#     pos_optim.step()\n",
    "\n",
    "#     if iteration % (n_iterations // 10) == 0:\n",
    "#         print(f\"Iteration {iteration} Loss: {loss.item()}\")\n",
    "#         # pos = constraint(pos.detach())\n",
    "\n",
    "#         fig, ax = plt.subplots(figsize=(FIGURE_SIZE_CNST, FIGURE_SIZE_CNST))\n",
    "\n",
    "#         show_pos = (pos.squeeze() + 1) / 2\n",
    "#         show_pos = show_pos * cfg.scenario.size\n",
    "#         layout = storage_to_layout(\n",
    "#             features=show_pos.numpy(force=True),\n",
    "#             config=cfg.scenario,\n",
    "#             representation_override=\"graph\",\n",
    "#         )\n",
    "#         warehouse = Warehouse(layout=layout, render_mode=\"rgb_array\")\n",
    "#         print(len(warehouse.shelves))\n",
    "#         im = warehouse.render()\n",
    "#         ax.imshow(im)\n",
    "#         ax.axis(\"off\")\n",
    "#         plt.show()\n",
    "#         warehouse.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d99571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = Generator(\n",
    "#     batch_size=10,\n",
    "#     generator_model_path=latest_checkpoint,\n",
    "#     scenario=cfg.scenario,\n",
    "#     guidance_wt=4,\n",
    "#     representation=\"graph\",\n",
    "# )\n",
    "# # guidance_model = model\n",
    "# guidance_model = model\n",
    "# guidance_model.eval()\n",
    "# operation = OptimizerDetails()\n",
    "# operation.lr = 0.003\n",
    "# operation.num_recurrences = 32\n",
    "# operation.backward_steps = 80\n",
    "# operation.projection_constraint = graph_projection_constraint(cfg.scenario)\n",
    "# # operation.print = True\n",
    "# # operation.print_every = 5\n",
    "# # operation.folder = \"test_diffusion\"\n",
    "\n",
    "\n",
    "# def show_batch(environment_batch, n: int = 8):\n",
    "#     layouts = []\n",
    "#     for image in environment_batch:\n",
    "#         layout = storage_to_layout(image, cfg.scenario)\n",
    "#         warehouse = Warehouse(layout=layout, render_mode=\"rgb_array\")\n",
    "#         layouts.append(warehouse.render())\n",
    "#         warehouse.close()\n",
    "\n",
    "#     fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
    "#     axs = axs.ravel()\n",
    "#     for ax in axs:\n",
    "#         ax.axis(\"off\")\n",
    "#     for i in range(n):\n",
    "#         axs[i].imshow(layouts[i])\n",
    "#     return fig, axs\n",
    "\n",
    "\n",
    "# environment_batch = generator.generate_batch(\n",
    "#     value=guidance_model,\n",
    "#     use_operation=True,\n",
    "#     operation_override=operation,\n",
    "# )\n",
    "\n",
    "\n",
    "# cfg.scenario.representation = \"graph\"\n",
    "# for env in environment_batch:\n",
    "#     layout = storage_to_layout(env, cfg.scenario)\n",
    "#     print(len(layout.reset_shelves()))\n",
    "# fig, axs = show_batch(environment_batch)\n",
    "# fig.suptitle(\"Guided Generation\")\n",
    "# fig.tight_layout()\n",
    "\n",
    "# X_batch = (\n",
    "#     torch.from_numpy(environment_batch).to(device=device, dtype=torch.float32)\n",
    "#     # .moveaxis((0, 1, 2, 3), (0, 2, 3, 1))\n",
    "# )\n",
    "# # X_batch = torch.cat([X_batch, goal_map.unsqueeze(0).expand(8, -1, -1, -1)], dim=1)\n",
    "# X_batch = (X_batch / (cfg.scenario.size - 1)) * 2 - 1\n",
    "# print(X_batch.shape)\n",
    "# print(guidance_model(X_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ad4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_co_design.rware.model.classifier import GNNClassifier\n",
    "\n",
    "model = GNNClassifier(cfg=cfg.scenario).to(device=device)\n",
    "\n",
    "# Test\n",
    "(pos, color), y = next(iter(train_dataloader))\n",
    "\n",
    "number_parameters = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Number of parameters: {number_parameters}\")\n",
    "assert model.predict((pos, color)).shape == y.shape\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_co_design.pretrain.rware.graph import visualize_warehouse_graph\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data = model.gnn.make_graph_batch_from_data(pos, color=color)[0].to_data_list()[6]\n",
    "visualize_warehouse_graph(data=data, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c690a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NUM_EPOCHS = 50\n",
    "RECOMPUTE = True\n",
    "\n",
    "model = GNNClassifier(\n",
    "    cfg=cfg.scenario, node_embedding_dim=512, edge_embedding_dim=32, num_layers=4\n",
    ").to(device=device)\n",
    "model_dir = os.path.join(working_dir, \"classifier_gnn.pt\")\n",
    "\n",
    "number_parameters = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Number of parameters: {number_parameters}\")\n",
    "\n",
    "\n",
    "if RECOMPUTE or not os.path.exists(model_dir):\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    with tqdm(range(TRAIN_NUM_EPOCHS)) as pbar:\n",
    "        for epoch in range(TRAIN_NUM_EPOCHS):\n",
    "            running_train_loss = 0\n",
    "            model.train()\n",
    "            for (pos, colors), y in train_dataloader:\n",
    "                optim.zero_grad()\n",
    "\n",
    "                batch_size = pos.shape[0]\n",
    "                y_pred = model.predict((pos, colors))\n",
    "                loss = criterion(y_pred.view(batch_size, -1), y.view(batch_size, -1))\n",
    "\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                running_train_loss += loss.item()\n",
    "            running_train_loss = running_train_loss / len(train_dataloader)\n",
    "\n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            running_eval_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for (pos, colors), y in eval_dataloader:\n",
    "                    y_pred = model.predict((pos, colors)).squeeze()\n",
    "                    batch_size = pos.shape[0]\n",
    "                    loss = criterion(\n",
    "                        y_pred.view(batch_size, -1), y.view(batch_size, -1)\n",
    "                    )\n",
    "\n",
    "                    running_eval_loss += loss.item()\n",
    "            running_eval_loss = running_eval_loss / len(eval_dataloader)\n",
    "\n",
    "            train_losses.append(running_train_loss)\n",
    "            eval_losses.append(running_eval_loss)\n",
    "            pbar.set_description(\n",
    "                f\" Train Loss {running_train_loss} Eval Loss {running_eval_loss}\"\n",
    "            )\n",
    "            pbar.update()\n",
    "\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
