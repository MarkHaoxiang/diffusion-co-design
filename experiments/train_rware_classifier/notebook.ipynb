{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb763ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from experiments.train_rware.main import TrainingConfig\n",
    "from diffusion_co_design.common import OUTPUT_DIR, get_latest_model, cuda\n",
    "\n",
    "from diffusion_co_design.rware.model.classifier import make_model\n",
    "from diffusion_co_design.rware.diffusion.transform import (\n",
    "    storage_to_layout,\n",
    "    graph_projection_constraint,\n",
    "    image_projection_constraint,\n",
    ")\n",
    "from diffusion_co_design.rware.diffusion.generator import Generator, OptimizerDetails\n",
    "from rware.warehouse import Warehouse\n",
    "\n",
    "from dataset import load_dataset, make_dataloader\n",
    "\n",
    "\n",
    "device = cuda\n",
    "training_dir = \"/home/markhaoxiang/.diffusion_co_design/experiments/train_rware/2025-04-18/00-36-30\"  # Four corners\n",
    "representation = \"image\"\n",
    "\n",
    "# Load latest model and config\n",
    "cfg = TrainingConfig.from_file(os.path.join(training_dir, \".hydra\", \"config.yaml\"))\n",
    "diffusion_dir = pretrain_dir = os.path.join(\n",
    "    OUTPUT_DIR, \"rware\", \"diffusion\", representation, cfg.scenario.name\n",
    ")\n",
    "latest_diffusion_checkpoint = get_latest_model(diffusion_dir, \"model\")\n",
    "\n",
    "# Make Dataset\n",
    "train_dataset, eval_dataset = load_dataset(\n",
    "    scenario=cfg.scenario,\n",
    "    training_dir=training_dir,\n",
    "    dataset_size=10_000,\n",
    "    num_workers=25,\n",
    "    test_proportion=0.2,\n",
    "    recompute=False,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataloader = make_dataloader(\n",
    "    train_dataset,\n",
    "    scenario=cfg.scenario,\n",
    "    batch_size=128,\n",
    "    representation=representation,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "eval_dataloader = make_dataloader(\n",
    "    eval_dataset,\n",
    "    scenario=cfg.scenario,\n",
    "    batch_size=128,\n",
    "    representation=representation,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "match representation:\n",
    "    case \"graph\":\n",
    "        model = make_model(\n",
    "            \"gnn-cnn\",\n",
    "            cfg.scenario,\n",
    "            model_kwargs={\"add_goal_positions\": False},\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        model.load_state_dict(\n",
    "            torch.load(\n",
    "                \"/home/markhaoxiang/.diffusion_co_design/experiments/train_rware_classifier/2025-04-22/18-45-03/checkpoints/classifier.pt\"\n",
    "            )\n",
    "        )\n",
    "    case \"image\":\n",
    "        model = make_model(\n",
    "            \"cnn\",\n",
    "            cfg.scenario,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        model.load_state_dict(\n",
    "            torch.load(\n",
    "                \"/home/markhaoxiang/.diffusion_co_design/experiments/train_rware_classifier/2025-04-23/09-58-41/checkpoints/classifier.pt\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b01e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.vision_transformer import VisionTransformer\n",
    "\n",
    "vt = VisionTransformer(\n",
    "    image_size=16,\n",
    "    patch_size=8,\n",
    "    num_classes=1,\n",
    "    num_heads=8,\n",
    "    hidden_dim=128,\n",
    "    mlp_dim=128,\n",
    "    num_layers=4,\n",
    ")\n",
    "print(sum(p.numel() for p in vt.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "model = make_model(\n",
    "    \"cnn\",\n",
    "    cfg.scenario,\n",
    "    device=device,\n",
    "    model_kwargs={\n",
    "        \"model_channels\": 64,\n",
    "        \"channel_mult\": (1, 2, 2, 2),\n",
    "        \"num_attention_head_channels\": 64,\n",
    "        \"resblock_updown\": True,\n",
    "        \"attention_resolutions\": (16, 8, 4),\n",
    "        \"depthwise_separable\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "def summarize_parameters(module: nn.Module, name: str = \"\", indent: int = 0):\n",
    "    \"\"\"Recursively summarize parameters in a PyTorch module.\"\"\"\n",
    "    total_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    submodules = list(module.named_children())\n",
    "\n",
    "    # Print current module\n",
    "    prefix = \" \" * (indent * 2)\n",
    "    print(f\"{prefix}{name or module.__class__.__name__}: {total_params:,} params\")\n",
    "\n",
    "    # Recurse into children\n",
    "    for child_name, child_module in submodules:\n",
    "        summarize_parameters(child_module, name=child_name, indent=indent + 1)\n",
    "\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "summarize_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4557b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1398bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663cf930",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].hist(\n",
    "    eval_dataset.dataset.env_returns.storage[\"episode_reward\"],\n",
    "    bins=100,\n",
    ")\n",
    "axs[0].set_title(\"Episode Reward\")\n",
    "axs[1].hist(\n",
    "    eval_dataset.dataset.env_returns.storage[\"expected_reward\"],\n",
    "    bins=100,\n",
    ")\n",
    "axs[1].set_title(\"Expected Reward (Critic)\")\n",
    "\n",
    "classifier_returns = []\n",
    "with torch.no_grad():\n",
    "    for x, _, _ in eval_dataloader:\n",
    "        classifier_returns.append(model(x.to(device)))\n",
    "classifier_returns = torch.cat(classifier_returns, dim=0)\n",
    "axs[2].hist(\n",
    "    classifier_returns.cpu().numpy(),\n",
    "    bins=100,\n",
    ")\n",
    "axs[2].set_title(\"Classifier Returns\")\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Mean Episode Reward: \",\n",
    "    train_dataset.dataset.env_returns.storage[\"episode_reward\"].mean(),\n",
    ")\n",
    "print(\n",
    "    \"Mean Expected Reward: \",\n",
    "    train_dataset.dataset.env_returns.storage[\"expected_reward\"].mean(),\n",
    ")\n",
    "print(\n",
    "    \"Mean Classifier Returns: \",\n",
    "    classifier_returns.mean(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99fd0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(\n",
    "    batch_size=10,\n",
    "    generator_model_path=latest_diffusion_checkpoint,\n",
    "    scenario=cfg.scenario,\n",
    "    guidance_wt=200 if representation == \"image\" else 5.0,\n",
    "    representation=representation,\n",
    ")\n",
    "guidance_model = model\n",
    "guidance_model.eval()\n",
    "\n",
    "operation = OptimizerDetails()\n",
    "match representation:\n",
    "    case \"graph\":\n",
    "        operation.lr = 0.01\n",
    "        operation.num_recurrences = 8\n",
    "        operation.backward_steps = 16\n",
    "        operation.projection_constraint = graph_projection_constraint(cfg.scenario)\n",
    "    case \"image\":\n",
    "        operation.num_recurrences = 8\n",
    "        operation.backward_steps = 0\n",
    "        operation.projection_constraint = image_projection_constraint(cfg.scenario)\n",
    "\n",
    "\n",
    "def show_batch(\n",
    "    environment_batch,\n",
    "    representation,\n",
    "    n: int = 8,\n",
    "):\n",
    "    layouts = []\n",
    "    for theta in environment_batch:\n",
    "        layout = storage_to_layout(theta, cfg.scenario, representation=representation)\n",
    "        warehouse = Warehouse(layout=layout, render_mode=\"rgb_array\")\n",
    "        layouts.append(warehouse.render())\n",
    "        warehouse.close()\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    axs = axs.ravel()\n",
    "    for ax in axs:\n",
    "        ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        axs[i].imshow(layouts[i])\n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "environment_batch = generator.generate_batch(\n",
    "    value=guidance_model,\n",
    "    use_operation=True,\n",
    "    operation_override=operation,\n",
    ")\n",
    "\n",
    "\n",
    "for env in environment_batch:\n",
    "    layout = storage_to_layout(env, cfg.scenario, representation=representation)\n",
    "    print(len(layout.reset_shelves()))\n",
    "fig, axs = show_batch(environment_batch, representation)\n",
    "fig.suptitle(\"Guided Generation\")\n",
    "fig.tight_layout()\n",
    "\n",
    "X_batch = torch.from_numpy(environment_batch).to(device=device, dtype=torch.float32)\n",
    "match representation:\n",
    "    case \"graph\":\n",
    "        X_batch = (X_batch / (cfg.scenario.size - 1)) * 2 - 1\n",
    "    case \"image\":\n",
    "        X_batch = X_batch * 2 - 1\n",
    "\n",
    "print(X_batch.shape)\n",
    "print(guidance_model(X_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c677a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE_SIZE_CNST = 2.5\n",
    "\n",
    "# test_layout = [next(iter(train_dataset))]\n",
    "# test_layout, _ = collate_fn(test_layout)\n",
    "# pos, color = test_layout\n",
    "\n",
    "# pos.requires_grad = True\n",
    "# pos_optim = torch.optim.Adam([pos], lr=0.01)\n",
    "\n",
    "# constraint = graph_projection_constraint(cfg.scenario)\n",
    "\n",
    "# n_iterations = 1000\n",
    "# for iteration in range(n_iterations):\n",
    "#     pos.requires_grad = True\n",
    "#     pos_optim.zero_grad()\n",
    "#     y_pred = model.predict((pos, color))\n",
    "#     loss = -y_pred.mean()\n",
    "#     loss.backward()\n",
    "#     pos_optim.step()\n",
    "\n",
    "#     if iteration % (n_iterations // 10) == 0:\n",
    "#         print(f\"Iteration {iteration} Loss: {loss.item()}\")\n",
    "#         # pos = constraint(pos.detach())\n",
    "\n",
    "#         fig, ax = plt.subplots(figsize=(FIGURE_SIZE_CNST, FIGURE_SIZE_CNST))\n",
    "\n",
    "#         show_pos = (pos.squeeze() + 1) / 2\n",
    "#         show_pos = show_pos * cfg.scenario.size\n",
    "#         layout = storage_to_layout(\n",
    "#             features=show_pos.numpy(force=True),\n",
    "#             config=cfg.scenario,\n",
    "#             representation_override=\"graph\",\n",
    "#         )\n",
    "#         warehouse = Warehouse(layout=layout, render_mode=\"rgb_array\")\n",
    "#         print(len(warehouse.shelves))\n",
    "#         im = warehouse.render()\n",
    "#         ax.imshow(im)\n",
    "#         ax.axis(\"off\")\n",
    "#         plt.show()\n",
    "#         warehouse.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ad4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_co_design.rware.model.classifier import GNNClassifier\n",
    "\n",
    "model = GNNClassifier(cfg=cfg.scenario).to(device=device)\n",
    "\n",
    "# Test\n",
    "(pos, color), y = next(iter(train_dataloader))\n",
    "\n",
    "number_parameters = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Number of parameters: {number_parameters}\")\n",
    "assert model.predict((pos, color)).shape == y.shape\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
