{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc00de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wfcrl import environments as envs\n",
    "import torch\n",
    "\n",
    "# Tensordict modules\n",
    "from tensordict.nn import set_composite_lp_aggregate, TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "\n",
    "# Data collection\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "# Env\n",
    "from torchrl.envs import TransformedEnv\n",
    "\n",
    "# Multi-agent network\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "\n",
    "# Loss\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "from torchrl.envs import (\n",
    "    PettingZooWrapper,\n",
    "    TransformedEnv,\n",
    "    CatTensors,\n",
    "    RewardSum,\n",
    "    Compose,\n",
    ")\n",
    "\n",
    "from pettingzoo.utils.conversions import aec_to_parallel\n",
    "\n",
    "from diffusion_co_design.utils import memory_management\n",
    "from diffusion_co_design.wfcrl.model import wfcrl_models\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 2_000\n",
    "n_iters = 30\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 10\n",
    "minibatch_size = 500\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 0.5  # Maximum norm for the gradients\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2  # clip value for PPO loss\n",
    "gamma = 0.99  # discount factor\n",
    "lmbda = 0.9  # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-4  # coefficient of the entropy term in the PPO loss\n",
    "\n",
    "# disable log-prob aggregation\n",
    "set_composite_lp_aggregate(False).set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = memory_management(\"gpu\").env_device\n",
    "env = PettingZooWrapper(\n",
    "    aec_to_parallel(envs.make(\"Dec_Turb3_Row1_Floris\")), device=device\n",
    ")\n",
    "observation_keys = [\n",
    "    (\"turbine\", \"observation\", x) for x in [\"wind_direction\", \"wind_speed\", \"yaw\"]\n",
    "]\n",
    "env = TransformedEnv(\n",
    "    env=env,\n",
    "    transform=Compose(\n",
    "        CatTensors(\n",
    "            in_keys=observation_keys,\n",
    "            out_key=(\"turbine\", \"observation_vec\"),\n",
    "        ),\n",
    "        RewardSum(in_keys=[env.reward_key], out_keys=[(\"turbine\", \"episode_reward\")]),\n",
    "    ),\n",
    ")\n",
    "env.reset()\n",
    "\n",
    "\n",
    "def make_policy(env, device: str):\n",
    "    policy_net = torch.nn.Sequential(\n",
    "        MultiAgentMLP(\n",
    "            n_agent_inputs=env.observation_spec[\"turbine\", \"observation_vec\"].shape[-1],\n",
    "            n_agent_outputs=2 * env.action_spec.shape[-1],\n",
    "            n_agents=env.num_agents,\n",
    "            centralised=False,\n",
    "            share_params=False,\n",
    "            device=device,\n",
    "            depth=2,\n",
    "            num_cells=256,\n",
    "            activation_class=torch.nn.Tanh,\n",
    "        ),\n",
    "        NormalParamExtractor(),\n",
    "    )\n",
    "    policy_module = TensorDictModule(\n",
    "        policy_net,\n",
    "        in_keys=[(\"turbine\", \"observation_vec\")],\n",
    "        out_keys=[(\"turbine\", \"loc\"), (\"turbine\", \"scale\")],\n",
    "    )\n",
    "    policy = ProbabilisticActor(\n",
    "        module=policy_module,\n",
    "        spec=env.action_spec_unbatched,\n",
    "        in_keys=[(\"turbine\", \"loc\"), (\"turbine\", \"scale\")],\n",
    "        out_keys=[env.action_key],\n",
    "        distribution_class=TanhNormal,\n",
    "        distribution_kwargs={\n",
    "            \"low\": env.full_action_spec_unbatched[env.action_key].space.low,\n",
    "            \"high\": env.full_action_spec_unbatched[env.action_key].space.high,\n",
    "        },\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=(\"turbine\", \"sample_log_prob\"),\n",
    "    )\n",
    "\n",
    "    critic_net = MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"turbine\", \"observation_vec\"].shape[-1],\n",
    "        n_agent_outputs=1,  # 1 value per agent\n",
    "        n_agents=env.num_agents,\n",
    "        centralised=True,\n",
    "        share_params=False,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    )\n",
    "\n",
    "    critic = TensorDictModule(\n",
    "        module=critic_net,\n",
    "        in_keys=[(\"turbine\", \"observation_vec\")],\n",
    "        out_keys=[(\"turbine\", \"state_value\")],\n",
    "    )\n",
    "\n",
    "    # Initialise\n",
    "    td = env.reset().to(device)\n",
    "    with torch.no_grad():\n",
    "        policy(td)\n",
    "        critic(td)\n",
    "\n",
    "    return policy, critic\n",
    "\n",
    "\n",
    "policy, critic = make_policy(env, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ")\n",
    "\n",
    "\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(\n",
    "        frames_per_batch, device=device\n",
    "    ),  # We store the frames_per_batch collected at each iteration\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=minibatch_size,  # We will sample minibatches of this size\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy,\n",
    "    critic_network=critic,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_coef=entropy_eps,\n",
    "    normalize_advantage=False,  # Important to avoid normalizing across the agent dimension\n",
    ")\n",
    "loss_module.set_keys(  # We have to tell the loss where to find the keys\n",
    "    reward=env.reward_key,\n",
    "    action=env.action_key,\n",
    "    value=(\"turbine\", \"state_value\"),\n",
    "    done=(\"turbine\", \"done\"),\n",
    "    terminated=(\"turbine\", \"terminated\"),\n",
    "    sample_log_prob=(\"turbine\", \"sample_log_prob\"),\n",
    ")\n",
    "\n",
    "\n",
    "loss_module.make_value_estimator(\n",
    "    ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
    ")  # We build GAE\n",
    "GAE = loss_module.value_estimator\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c1a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
    "\n",
    "episode_reward_mean_list = []\n",
    "for tensordict_data in collector:\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"turbine\", \"done\"),\n",
    "        tensordict_data.get((\"next\", \"done\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"turbine\", \"terminated\"),\n",
    "        tensordict_data.get((\"next\", \"terminated\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    # We need to expand the done and terminated to match the reward shape (this is expected by the value estimator)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        GAE(\n",
    "            tensordict_data,\n",
    "            params=loss_module.critic_network_params,\n",
    "            target_params=loss_module.target_critic_network_params,\n",
    "        )  # Compute GAE and add it to the data\n",
    "\n",
    "    data_view = tensordict_data.reshape(-1)  # Flatten the batch size to shuffle data\n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            )  # Optional\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    # Logging\n",
    "    done = tensordict_data.get((\"next\", \"turbine\", \"done\"))\n",
    "    episode_reward_mean = (\n",
    "        tensordict_data.get((\"next\", \"turbine\", \"episode_reward\"))[done].mean().item()\n",
    "    )\n",
    "    episode_reward_mean_list.append(episode_reward_mean)\n",
    "    pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
    "    pbar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward_mean_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
