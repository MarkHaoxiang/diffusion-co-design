defaults:
  - _self_
  - metadata/training@_here_

experiment_name: wfcrl_test
device:
  device_management: "gpu"
  gpu_id: 0
  max_gpu_memory: 1.0
normalize_reward: true
scenario:
  n_turbines: 10
  max_steps: 150
  map_x_length: 500
  map_y_length: 500
  min_distance_between_turbines: 100
policy:
  node_hidden_size: 32
  edge_hidden_size: 16
  mlp_hidden_size: 32
  backbone_depth: 3
  head_depth: 2
ppo:
  n_iters: 5
  n_epochs: 4
  minibatch_size: 150
  n_mini_batches: 2
  clip_epsilon: 0.2
  gamma: 0.99
  lmbda: 0.95
  actor_lr: 3e-4
  critic_lr: 3e-4
  lr_scheduler_enabled: true
  max_grad_norm: 1.0
  entropy_eps: 1e-4
  normalise_advantage: true
designer:
  type: diffusion
  environment_repeats: 1
  model:
    node_emb_size: 32
    edge_emb_size: 16
    depth: 1
  buffer_size: 1024
  diffusion_early_start: 1
  train_early_start: 0
  batch_size: 32
  weight_decay: 0
  lr: 1e-4
  distill_enable: false
  loss_criterion: huber
  operation:
    num_reccurences: 1
    backward_lr: 0.01
    backward_steps: 0
    forward_guidance: 2.0

logging:
  mode: disabled
  # mode: online
  # mode: offline
  evaluation_interval: 1
  evaluation_episodes: 1
  checkpoint_interval: 20