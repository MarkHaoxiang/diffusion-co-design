defaults:
  - _self_
  - metadata/training@_here_

experiment_name: wfcrl_fixed
device:
  device_management: "gpu"
  gpu_id: 0
  max_gpu_memory: 1.0
normalize_reward: true
scenario:
  n_turbines: 10
  max_steps: 150
  map_x_length: 500
  map_y_length: 500
  min_distance_between_turbines: 100
policy:
  model_type: "gnn"
  initial_std: 0.3
  mlp_hidden_size: 64
  mlp_depth: 2
  policy_node_hidden_size: 64
  policy_edge_hidden_size: 32
  policy_head_hidden_size: 64
  policy_gnn_depth: 1
  policy_head_depth: 2
  critic_node_hidden_size: 64
  critic_edge_hidden_size: 32
  critic_gnn_depth: 3
ppo:
  n_iters: 2000
  n_epochs: 20
  minibatch_size: 150
  n_mini_batches: 10
  clip_epsilon: 0.2
  gamma: 0.99
  lmbda: 0.95
  actor_lr: 3e-4
  critic_lr: 3e-4
  min_critic_lr: 1e-4
  lr_scheduler_enabled: true
  max_grad_norm: 1.0
  entropy_eps: 1e-5
  normalise_advantage: true

logging:
  # mode: disabled
  mode: online
  # mode: offline
  evaluation_interval: 50
  evaluation_episodes: 5
  checkpoint_interval: 50