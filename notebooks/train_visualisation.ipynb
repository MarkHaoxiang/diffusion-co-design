{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from IPython.display import Video\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "from gymnasium.utils.save_video import save_video\n",
    "import torch\n",
    "from tensordict import TensorDict\n",
    "from torchrl.envs import EnvBase\n",
    "from torchrl.data import ReplayBuffer, LazyTensorStorage, SamplerWithoutReplacement\n",
    "from matplotlib import pyplot as plt\n",
    "from guided_diffusion.script_util import create_classifier, classifier_defaults\n",
    "from rware.warehouse import Warehouse\n",
    "import pickle as pkl\n",
    "from diffusion_co_design.rware.env import create_env, create_batched_env\n",
    "from diffusion_co_design.rware.model import rware_models\n",
    "from diffusion_co_design.utils import (\n",
    "    omega_to_pydantic,\n",
    "    get_latest_model,\n",
    "    cuda,\n",
    "    OUTPUT_DIR,\n",
    ")\n",
    "from diffusion_co_design.pretrain.rware.transform import storage_to_layout\n",
    "from diffusion_co_design.bin.train_rware import TrainingConfig, DesignerRegistry\n",
    "from diffusion_co_design.pretrain.rware.generator import Generator, GeneratorConfig\n",
    "\n",
    "FIGURE_SIZE_CNST = 3\n",
    "\n",
    "# Parameters\n",
    "training_dir = \"/home/markhaoxiang/.diffusion_co_design/training/2025-02-17/09-40-27\"\n",
    "device = cuda\n",
    "\n",
    "# Get latest policy\n",
    "checkpoint_dir = os.path.join(training_dir, \"checkpoints\")\n",
    "latest_policy = get_latest_model(checkpoint_dir, \"policy_\")\n",
    "\n",
    "# Get config\n",
    "hydra_dir = os.path.join(training_dir, \".hydra\")\n",
    "training_config = os.path.join(hydra_dir, \"config.yaml\")\n",
    "cfg = omega_to_pydantic(OmegaConf.load(training_config), TrainingConfig)\n",
    "\n",
    "# Create environment\n",
    "cache_dir = os.path.join(OUTPUT_DIR, \".tmp\")\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "os.makedirs(cache_dir)\n",
    "master_designer, env_designer = DesignerRegistry.get(\n",
    "    \"random\",\n",
    "    cfg.scenario,\n",
    "    cache_dir,\n",
    "    environment_batch_size=32,\n",
    "    device=device,\n",
    ")\n",
    "env = create_env(cfg.scenario, env_designer, render=True, device=device)\n",
    "policy, _ = rware_models(env, cfg.policy, device=device)\n",
    "policy.load_state_dict(torch.load(latest_policy))\n",
    "\n",
    "\n",
    "def view_video(env: EnvBase, policy):\n",
    "    frames = []\n",
    "    video_out = os.path.join(cache_dir, \"video/rl-video-episode-0.mp4\")\n",
    "\n",
    "    def append_frames(env, td):\n",
    "        return frames.append(env.render())\n",
    "\n",
    "    env.rollout(\n",
    "        max_steps=cfg.scenario.max_steps,\n",
    "        policy=policy,\n",
    "        callback=append_frames,\n",
    "        auto_cast_to_device=True,\n",
    "    )\n",
    "\n",
    "    save_video(\n",
    "        frames=frames,\n",
    "        video_folder=os.path.join(cache_dir, \"video\"),\n",
    "        fps=10,\n",
    "    )\n",
    "\n",
    "    return lambda: Video(filename=video_out, embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view_video(env, policy)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the value function is able to discern good environments!\n",
    "\n",
    "NUM_PARALLEL_COLLECTION = 25\n",
    "DATASET_SIZE = 10_000\n",
    "# DATASET_SIZE = 4\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "collection_env = create_batched_env(\n",
    "    num_environments=NUM_PARALLEL_COLLECTION,\n",
    "    scenario=cfg.scenario,\n",
    "    designer=env_designer,\n",
    "    is_eval=False,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "env_returns = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=DATASET_SIZE),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "for _ in tqdm(range(DATASET_SIZE // NUM_PARALLEL_COLLECTION)):\n",
    "    rollout = collection_env.rollout(\n",
    "        max_steps=cfg.scenario.max_steps, policy=policy, auto_cast_to_device=True\n",
    "    )\n",
    "    done = rollout.get((\"next\", \"done\"))\n",
    "    X = rollout.get(\"state\")[done.squeeze()]\n",
    "    y = rollout.get((\"next\", \"agents\", \"episode_reward\")).mean(-2)[done]\n",
    "    data = TensorDict({\"env\": X, \"episode_reward\": y}, batch_size=len(y))\n",
    "    env_returns.extend(data)\n",
    "del rollout, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GUIDANCE_WT = 50\n",
    "TRAIN_NUM_ITERATIONS = 10000\n",
    "VALUE_LR = 3e-4\n",
    "VALUE_WEIGHT_DECAY = 0.05\n",
    "OUTPUT_CHANNELS = 1\n",
    "\n",
    "# Create value model\n",
    "pretrain_dir = os.path.join(OUTPUT_DIR, \"diffusion_pretrain\", cfg.scenario.name)\n",
    "latest_checkpoint = get_latest_model(pretrain_dir, \"model\")\n",
    "\n",
    "gen_cfg = GeneratorConfig(\n",
    "    batch_size=8,\n",
    "    generator_model_path=latest_checkpoint,\n",
    "    size=cfg.scenario.size,\n",
    "    num_channels=OUTPUT_CHANNELS,\n",
    ")\n",
    "generator = Generator(gen_cfg, guidance_wt=GUIDANCE_WT)\n",
    "\n",
    "model_dict = classifier_defaults()\n",
    "model_dict[\"image_size\"] = cfg.scenario.size\n",
    "model_dict[\"image_channels\"] = OUTPUT_CHANNELS\n",
    "model_dict[\"classifier_width\"] = 256\n",
    "model_dict[\"classifier_depth\"] = 2\n",
    "model_dict[\"classifier_attention_resolutions\"] = \"16, 8, 4\"\n",
    "model_dict[\"output_dim\"] = 1\n",
    "\n",
    "model = create_classifier(**model_dict).to(device)\n",
    "\n",
    "# Train\n",
    "optim = torch.optim.Adam(\n",
    "    model.parameters(), lr=VALUE_LR, weight_decay=VALUE_WEIGHT_DECAY\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "with tqdm(range(TRAIN_NUM_ITERATIONS)) as pbar:\n",
    "    for _ in range(TRAIN_NUM_ITERATIONS):\n",
    "        optim.zero_grad()\n",
    "        sample = env_returns.sample(batch_size=BATCH_SIZE)\n",
    "        X_batch = sample.get(\"env\").to(dtype=torch.float32, device=device)\n",
    "        y_batch = sample.get(\"episode_reward\").to(dtype=torch.float32, device=device)\n",
    "        t, _ = generator.schedule_sampler.sample(len(X_batch), device)\n",
    "        X_batch = generator.diffusion.q_sample(X_batch, t)\n",
    "        y_pred = model(X_batch, t).squeeze()\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        pbar.set_description(f\"Loss {loss.item()}\")\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "env_returns_sorted_index = torch.argsort(\n",
    "    env_returns.storage[\"episode_reward\"], descending=True\n",
    ")\n",
    "best_5 = env_returns_sorted_index[:5]\n",
    "worst_5 = env_returns_sorted_index[-5:]\n",
    "\n",
    "fig, axs = plt.subplots(2, 5)\n",
    "fig.set_size_inches(5 * FIGURE_SIZE_CNST, 2 * FIGURE_SIZE_CNST)\n",
    "\n",
    "for i, idx in enumerate(best_5):\n",
    "    ax = axs[0, i]\n",
    "    layout = storage_to_layout(\n",
    "        env_returns.storage[\"env\"][idx], cfg.scenario.agent_idxs, cfg.scenario.goal_idxs\n",
    "    )\n",
    "    print(env_returns.storage[\"episode_reward\"][idx])\n",
    "    warehouse = Warehouse(layout=layout, render_mode=\"rgb_array\")\n",
    "    im = warehouse.render()\n",
    "    ax.imshow(im)\n",
    "    warehouse.close()\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "print(\"===\")\n",
    "\n",
    "for i, idx in enumerate(worst_5):\n",
    "    ax = axs[1, i]\n",
    "    layout = storage_to_layout(\n",
    "        env_returns.storage[\"env\"][idx], cfg.scenario.agent_idxs, cfg.scenario.goal_idxs\n",
    "    )\n",
    "    print(env_returns.storage[\"episode_reward\"][idx])\n",
    "    warehouse = Warehouse(layout=layout, render_mode=\"rgb_array\")\n",
    "    im = warehouse.render()\n",
    "    ax.imshow(im)\n",
    "    warehouse.close()\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_5_env = env_returns.storage[\"env\"][best_5].to(device=device, dtype=torch.float32)\n",
    "model(best_5_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(env_returns.storage[\"env\"][worst_5].to(device=device, dtype=torch.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
