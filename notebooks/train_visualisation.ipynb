{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import numpy as np\n",
    "from IPython.display import Video\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "from gymnasium.utils.save_video import save_video\n",
    "import torch\n",
    "from tensordict import TensorDict\n",
    "from torchrl.envs import EnvBase\n",
    "from torchrl.data import ReplayBuffer, LazyTensorStorage, SamplerWithoutReplacement\n",
    "from matplotlib import pyplot as plt\n",
    "from guided_diffusion.script_util import create_classifier, classifier_defaults\n",
    "from diffusion_co_design.pretrain.rware.transform import storage_to_layout\n",
    "from rware.warehouse import Warehouse\n",
    "from diffusion_co_design.rware.env import create_env, create_batched_env\n",
    "from diffusion_co_design.rware.model import rware_models\n",
    "from diffusion_co_design.utils import (\n",
    "    omega_to_pydantic,\n",
    "    get_latest_model,\n",
    "    cuda,\n",
    "    OUTPUT_DIR,\n",
    ")\n",
    "from diffusion_co_design.pretrain.rware.transform import storage_to_layout\n",
    "from diffusion_co_design.bin.train_rware import TrainingConfig, DesignerRegistry\n",
    "from diffusion_co_design.pretrain.rware.generator import (\n",
    "    Generator,\n",
    "    GeneratorConfig,\n",
    "    OptimizerDetails,\n",
    ")\n",
    "\n",
    "FIGURE_SIZE_CNST = 3\n",
    "CACHE_ID = \"train_visualisation\"\n",
    "RECOMPUTE = False\n",
    "\n",
    "# Parameters\n",
    "training_dir = \"/home/markhaoxiang/.diffusion_co_design/training/2025-02-23/21-11-20\"\n",
    "device = cuda\n",
    "\n",
    "# Get latest policy\n",
    "checkpoint_dir = os.path.join(training_dir, \"checkpoints\")\n",
    "latest_policy = get_latest_model(checkpoint_dir, \"policy_\")\n",
    "\n",
    "# Get config\n",
    "hydra_dir = os.path.join(training_dir, \".hydra\")\n",
    "training_config = os.path.join(hydra_dir, \"config.yaml\")\n",
    "cfg = omega_to_pydantic(OmegaConf.load(training_config), TrainingConfig)\n",
    "\n",
    "# Create environment\n",
    "cache_dir = os.path.join(OUTPUT_DIR, \".tmp\", CACHE_ID)\n",
    "if RECOMPUTE and os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "master_designer, env_designer = DesignerRegistry.get(\n",
    "    \"random\",\n",
    "    cfg.scenario,\n",
    "    cache_dir,\n",
    "    environment_batch_size=32,\n",
    "    device=device,\n",
    ")\n",
    "agent_idxs = cfg.scenario.agent_idxs\n",
    "goal_idxs = cfg.scenario.goal_idxs\n",
    "env = create_env(cfg.scenario, env_designer, render=True, device=device)\n",
    "policy, _ = rware_models(env, cfg.policy, device=device)\n",
    "policy.load_state_dict(torch.load(latest_policy))\n",
    "\n",
    "\n",
    "def view_video(env: EnvBase, policy):\n",
    "    frames = []\n",
    "    video_out = os.path.join(cache_dir, \"video/rl-video-episode-0.mp4\")\n",
    "\n",
    "    def append_frames(env, td):\n",
    "        return frames.append(env.render())\n",
    "\n",
    "    env.rollout(\n",
    "        max_steps=cfg.scenario.max_steps,\n",
    "        policy=policy,\n",
    "        callback=append_frames,\n",
    "        auto_cast_to_device=True,\n",
    "    )\n",
    "\n",
    "    save_video(\n",
    "        frames=frames,\n",
    "        video_folder=os.path.join(cache_dir, \"video\"),\n",
    "        fps=10,\n",
    "    )\n",
    "\n",
    "    return lambda: Video(filename=video_out, embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the value function is able to discern good environments!\n",
    "\n",
    "NUM_PARALLEL_COLLECTION = 25\n",
    "DATASET_SIZE = 10_000\n",
    "# DATASET_SIZE = 4\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "collection_env = create_batched_env(\n",
    "    num_environments=NUM_PARALLEL_COLLECTION,\n",
    "    scenario=cfg.scenario,\n",
    "    designer=env_designer,\n",
    "    is_eval=False,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "env_returns = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(max_size=DATASET_SIZE),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "if RECOMPUTE:\n",
    "    for _ in tqdm(range(DATASET_SIZE // NUM_PARALLEL_COLLECTION)):\n",
    "        rollout = collection_env.rollout(\n",
    "            max_steps=cfg.scenario.max_steps, policy=policy, auto_cast_to_device=True\n",
    "        )\n",
    "        done = rollout.get((\"next\", \"done\"))\n",
    "        X = rollout.get(\"state\")[done.squeeze()]\n",
    "        y = rollout.get((\"next\", \"agents\", \"episode_reward\")).mean(-2)[done]\n",
    "        data = TensorDict({\"env\": X, \"episode_reward\": y}, batch_size=len(y))\n",
    "        env_returns.extend(data)\n",
    "    del rollout, done\n",
    "\n",
    "env_returns_path = os.path.join(cache_dir, \"env_returns\")\n",
    "if RECOMPUTE:\n",
    "    env_returns.dumps(env_returns_path)\n",
    "else:\n",
    "    env_returns.loads(env_returns_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GUIDANCE_WT = 50\n",
    "TRAIN_NUM_EPOCHS = 80\n",
    "VALUE_LR = 3e-4\n",
    "VALUE_WEIGHT_DECAY = 0.05\n",
    "OUTPUT_CHANNELS = 1\n",
    "RECOMPUTE = False\n",
    "\n",
    "ITERATIONS_PER_EPOCH = math.ceil(DATASET_SIZE / BATCH_SIZE)\n",
    "\n",
    "goal_map = np.zeros((cfg.scenario.size, cfg.scenario.size))\n",
    "for goal in goal_idxs:\n",
    "    goal_map[goal // cfg.scenario.size, goal % cfg.scenario.size] = 1\n",
    "goal_map = (\n",
    "    torch.from_numpy(goal_map).unsqueeze(0).to(device=device, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "# Create value model\n",
    "pretrain_dir = os.path.join(OUTPUT_DIR, \"diffusion_pretrain\", cfg.scenario.name)\n",
    "latest_checkpoint = get_latest_model(pretrain_dir, \"model\")\n",
    "\n",
    "gen_cfg = GeneratorConfig(\n",
    "    batch_size=8,\n",
    "    generator_model_path=latest_checkpoint,\n",
    "    size=cfg.scenario.size,\n",
    "    num_channels=OUTPUT_CHANNELS,\n",
    ")\n",
    "generator = Generator(gen_cfg, guidance_wt=GUIDANCE_WT)\n",
    "\n",
    "model_dict = classifier_defaults()\n",
    "model_dict[\"image_size\"] = cfg.scenario.size\n",
    "model_dict[\"image_channels\"] = OUTPUT_CHANNELS + 1\n",
    "model_dict[\"classifier_width\"] = 256\n",
    "model_dict[\"classifier_depth\"] = 2\n",
    "model_dict[\"classifier_attention_resolutions\"] = \"16, 8, 4\"\n",
    "model_dict[\"output_dim\"] = 1\n",
    "\n",
    "model = create_classifier(**model_dict).to(device)\n",
    "\n",
    "# Train\n",
    "optim = torch.optim.Adam(\n",
    "    model.parameters(), lr=VALUE_LR, weight_decay=VALUE_WEIGHT_DECAY\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "if RECOMPUTE:\n",
    "    model.train()\n",
    "    losses = []\n",
    "    with tqdm(range(TRAIN_NUM_EPOCHS)) as pbar:\n",
    "        for epoch in range(TRAIN_NUM_EPOCHS):\n",
    "            running_loss = 0\n",
    "            for _ in range(ITERATIONS_PER_EPOCH):\n",
    "                optim.zero_grad()\n",
    "                sample = env_returns.sample(batch_size=BATCH_SIZE)\n",
    "                X_batch = sample.get(\"env\").to(dtype=torch.float32, device=device)\n",
    "                # Add goal map\n",
    "                goal_map_batch = goal_map.expand(X_batch.shape[0], -1, -1).unsqueeze(1)\n",
    "                X_batch = torch.cat([X_batch, goal_map_batch], dim=1)\n",
    "\n",
    "                # Normalisation\n",
    "                X_batch = X_batch * 2 - 1\n",
    "                y_batch = sample.get(\"episode_reward\").to(\n",
    "                    dtype=torch.float32, device=device\n",
    "                )\n",
    "                # t, _ = generator.schedule_sampler.sample(len(X_batch), device)\n",
    "                # X_batch = generator.diffusion.q_sample(X_batch, t)\n",
    "                # y_pred = model(X_batch, t).squeeze()\n",
    "                y_pred = model(X_batch).squeeze()\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "            running_loss = running_loss / ITERATIONS_PER_EPOCH\n",
    "            losses.append(running_loss)\n",
    "            pbar.set_description(f\"Loss {running_loss}\")\n",
    "            pbar.update()\n",
    "    torch.save(model.state_dict(), \"train_visualisation_classifier.pt\")\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\"train_visualisation_classifier.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Value Function Train loss\")\n",
    "min(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "env_returns_sorted_index = torch.argsort(\n",
    "    env_returns.storage[\"episode_reward\"], descending=True\n",
    ")\n",
    "best_5 = env_returns_sorted_index[:5]\n",
    "worst_5 = env_returns_sorted_index[-5:]\n",
    "\n",
    "fig, axs = plt.subplots(2, 5)\n",
    "fig.set_size_inches(5 * FIGURE_SIZE_CNST, 2 * FIGURE_SIZE_CNST)\n",
    "\n",
    "for i, idx in enumerate(best_5):\n",
    "    ax = axs[0, i]\n",
    "    layout = storage_to_layout(\n",
    "        env_returns.storage[\"env\"][idx], cfg.scenario.agent_idxs, cfg.scenario.goal_idxs\n",
    "    )\n",
    "    print(env_returns.storage[\"episode_reward\"][idx])\n",
    "    warehouse = Warehouse(layout=layout, render_mode=\"rgb_array\")\n",
    "    im = warehouse.render()\n",
    "    ax.imshow(im)\n",
    "    warehouse.close()\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "print(\"===\")\n",
    "\n",
    "for i, idx in enumerate(worst_5):\n",
    "    ax = axs[1, i]\n",
    "    layout = storage_to_layout(\n",
    "        env_returns.storage[\"env\"][idx], cfg.scenario.agent_idxs, cfg.scenario.goal_idxs\n",
    "    )\n",
    "    print(env_returns.storage[\"episode_reward\"][idx])\n",
    "    warehouse = Warehouse(layout=layout, render_mode=\"rgb_array\")\n",
    "    im = warehouse.render()\n",
    "    ax.imshow(im)\n",
    "    warehouse.close()\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "N = 50\n",
    "\n",
    "model.eval()\n",
    "good_envs = env_returns.storage[\"env\"][env_returns_sorted_index[:N]].to(\n",
    "    device=device, dtype=torch.float32\n",
    ")\n",
    "goal_map_batch = goal_map.expand(good_envs.shape[0], -1, -1).unsqueeze(1)\n",
    "good_envs = torch.cat([good_envs, goal_map_batch], dim=1)\n",
    "print(f\"Good environments: {model(good_envs * 2 - 1).mean()}\")\n",
    "\n",
    "bad_envs = env_returns.storage[\"env\"][env_returns_sorted_index[-N:]].to(\n",
    "    device=device, dtype=torch.float32\n",
    ")\n",
    "bad_envs = torch.cat([bad_envs, goal_map_batch], dim=1)\n",
    "print(f\"Bad environments: {model(bad_envs * 2 - 1).mean()}\")\n",
    "\n",
    "del good_envs\n",
    "del bad_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_idxs = [1, 88, 132, 233, 162]\n",
    "goal_idxs = [39, 185, 237, 238, 159]\n",
    "\n",
    "\n",
    "generator = Generator(gen_cfg, guidance_wt=50)\n",
    "model.eval()\n",
    "operation = OptimizerDetails()\n",
    "operation.num_recurrences = 8\n",
    "operation.backward_steps = 0\n",
    "operation.operated_image = goal_map * 2 - 1\n",
    "\n",
    "\n",
    "def show_batch(environment_batch, n: int = 8):\n",
    "    layouts = []\n",
    "    for image in environment_batch:\n",
    "        layout = storage_to_layout(image, agent_idxs, goal_idxs)\n",
    "        warehouse = Warehouse(layout=layout, render_mode=\"rgb_array\")\n",
    "        layouts.append(warehouse.render())\n",
    "        warehouse.close()\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    axs = axs.ravel()\n",
    "    for ax in axs:\n",
    "        ax.axis(\"off\")\n",
    "    for i in range(n):\n",
    "        axs[i].imshow(layouts[i])\n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "environment_batch = generator.generate_batch(\n",
    "    value=model, use_operation=True, operation_override=operation\n",
    ")\n",
    "\n",
    "\n",
    "for env in environment_batch:\n",
    "    layout = storage_to_layout(env, agent_idxs, goal_idxs)\n",
    "    print(len(layout.reset_shelves()))\n",
    "fig, axs = show_batch(environment_batch)\n",
    "fig.suptitle(\"Guided Generation\")\n",
    "fig.tight_layout()\n",
    "\n",
    "X_batch = (\n",
    "    torch.from_numpy(environment_batch)\n",
    "    .to(device=device, dtype=torch.float32)\n",
    "    .moveaxis((0, 1, 2, 3), (0, 2, 3, 1))\n",
    ")\n",
    "X_batch = torch.cat([X_batch, goal_map.unsqueeze(0).expand(8, -1, -1, -1)], dim=1)\n",
    "X_batch = (X_batch * 2) - 1\n",
    "print(model(X_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counterfactual: randomly generated environments\n",
    "\n",
    "environment_batch = generator.generate_batch()\n",
    "\n",
    "\n",
    "for env in environment_batch:\n",
    "    layout = storage_to_layout(env, agent_idxs, goal_idxs)\n",
    "    print(len(layout.reset_shelves()))\n",
    "fig, axs = show_batch(environment_batch)\n",
    "fig.suptitle(\"Guided Generation\")\n",
    "fig.tight_layout()\n",
    "\n",
    "X_batch = (\n",
    "    torch.from_numpy(environment_batch)\n",
    "    .to(device=device, dtype=torch.float32)\n",
    "    .moveaxis((0, 1, 2, 3), (0, 2, 3, 1))\n",
    ")\n",
    "X_batch = torch.cat([X_batch, goal_map.unsqueeze(0).expand(8, -1, -1, -1)], dim=1)\n",
    "X_batch = (X_batch * 2) - 1\n",
    "print(model(X_batch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
